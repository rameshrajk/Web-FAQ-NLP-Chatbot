# -*- coding: utf-8 -*-
"""Copy of CS4395Chatbot_rrk170000_kdt170001_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fsibCr1Jqs6wjxxKtsMujjsmkFhwQAbr
"""

# imports
from bs4 import BeautifulSoup
from nltk import sent_tokenize
from nltk import tokenize
from nltk.corpus import stopwords
from textblob import TextBlob
import nltk
import os
import random
import re
import requests
import string
import sys
nltk.download("all")

"""Web Scraping"""

url = 'https://en.wikipedia.org/wiki/Kanye_West'
pltxt = requests.get(url).text
s = BeautifulSoup(pltxt, "html.parser")
links = []
links.append(url)
CWD: str = os.getcwd()

keyws = ["kanye", "west", "ye", "yeezy", "pablo", "donda", "fantasy", "registration", "dropout", "graduation"]

# related links
for link in s.findAll('a', attrs={'href': re.compile("^http://")}):
    link_url = link.get('href')
    if any(substring in link_url for substring in keyws):
        if link_url not in links:
            links.append(link_url)

# related wiki pages
for link in s.findAll('a', attrs={'href': re.compile("/wiki/")}):
    link_url = link.get('href')
    if any(substring in link_url.lower() for substring in keyws):
        # filter images, and non-english pages
        if "File:" in link_url:
            continue
        if link_url.startswith('/wiki/'):
            link_url = "https://en.wikipedia.org" + link_url
        if "wikipedia" in link_url and not link_url.startswith('https://en.'):
            continue
        if link_url not in links:
            links.append(link_url)

file_names = []
# saved text from urls
for x in range(len(links)):
    try:
        link_code = requests.get(links[x])
    except:
        continue
    # parse with BS
    soup2 = BeautifulSoup(link_code.content, 'html.parser')
    file_write = os.path.join(CWD, "raw_texts")
    filename = str(x) + ".txt"
    file_names.append(filename)
    file_write = os.path.join(file_write, filename)
    os.makedirs(os.path.dirname(file_write), exist_ok=True)
    with open(file_write, 'w', encoding='utf-8') as f_out:
        for text in soup2.findAll('p'):
            f_out.write(text.getText())

"""Clean Up"""

# download nltk
import nltk
nltk.download('punkt')
# ---

def readFile(file_name):
    """
      - open and read file
      - return contents of file
    """

    # open the file and store in variable
    with open(os.path.join(CWD, file_name), 'r') as file:
        raw_text = file.read()

    return raw_text
# ---

# write sentences to new file


def writeFile(sent_tokens, file_name):
    """
      - write sentences to a file
    """

    file_write = os.path.join(CWD, "new_files")
  
    # new file name for new file
    new_file_name = 'new_' + file_name

    file_write = os.path.join(file_write, new_file_name)
    os.makedirs(os.path.dirname(file_write), exist_ok=True)

    # open the file to be written
    with open(file_write, 'w') as file:
        # loop through all sentences
        for sent in sent_tokens:
            file.write(sent)
        file.close()

    return new_file_name
# ---

# store raw text from file
raw_text = "" 

# store new file names
new_file_names = [] 

## file_names assumed to be a list of file names which contain url info
for file_name in file_names:
  # add new file_name to list of new file names
  

  # store contents of file
  raw_text = readFile('raw_texts/' + file_name)

  # remove tabs and newlines
  text = raw_text.replace('[\n\t]' , '')

  # tokenize sentences
  sent_tokens = sent_tokenize(text)

  # write to file
  new_file_names.append(writeFile(sent_tokens, file_name))

"""Extract Words"""

# download stopwords from nltk
nltk.download('stopwords')
# ---

from nltk import word_tokenize
from nltk.corpus import stopwords 

# preprocess function for the file
def preprocess(raw_text):
  """
    - remove punctuation
    - lowercase the text
    - remove stopwords
  """

  # remove punctuation and lower-case the text
  text = re.sub(r'[.?!,:;()|#<>@*%$&=_`~\â€”\[\]\\\-\'\"\/]' , '', raw_text.lower())

  # tokenize words to remove stopwords
  word_tokens = word_tokenize(text)

  # remove stopwords
  stop_words = set(stopwords.words('english'))
  final_tokens = [t for t in word_tokens if not t in stop_words]

  return final_tokens
# ---

# get the frequency of a term
def getTermFrequency(main_term, all_terms):
  """
    - find number of occurences of main_term
    - calculate and return the term frequency
  """
  
  # store num of times term is found
  num_term = 0

  # count occurences of main_term
  for term in all_terms:
    if term == main_term:
      num_term += 1
  
  # calculate term frequency
  term_frequency = num_term / len(all_terms)

  return term_frequency
# ---

# get the N most important terms
def getImportantTerms(all_terms, N):
  """
    - use term frequency to return a list of the N most important words
  """

  # store important terms
  termDict = {}

  # loop through all of the unique terms
  for term in set(all_terms):

    # calculate the term frequency of new term
    tf = getTermFrequency(term, all_terms)

    # if there are N important terms
    if len(termDict) == N:
      # get the least frequent important term
      least_term = min(termDict, key=termDict.get)

      # compare tf to lowest tf of termDict
      if tf > termDict[least_term]: 
        termDict[term] = tf
        del termDict[least_term] # remove previous term
    else:
      # add to important terms
      termDict[term] = tf 

  return termDict
# ---

# store all text from all files
all_text = ""


# store important terms
importantTerms = {}

for file_name in new_file_names:
  # read in raw text from the file
  all_text = readFile("new_files/" + file_name)

# get the important terms from the preprocessed text
importantTerms = getImportantTerms(preprocess(all_text), 50)

# print the important terms sorted by term frequency
for k in sorted(importantTerms, key=lambda k:importantTerms[k], reverse=True):
  print(k, importantTerms[k])

"""Knowledge Base"""

# the important terms
fact_words = list(importantTerms.keys())

path = os.path.join("new_files")
# add to knowledge base
for file in os.listdir(path):
    filename = path + "//" + os.fsdecode(file)
    with open(filename, "r", encoding='utf=8') as f_in:
        text = f_in.read()
        tokens = nltk.sent_tokenize(text)
        file_write = os.path.join(CWD, "files")
        file_write = os.path.join(file_write, "knowledge_base.txt")
        # dictionary holds the knowledge base
        os.makedirs(os.path.dirname(file_write), exist_ok=True)
        with open(file_write, "a", encoding='utf=8') as f_out:
            for token in tokens:
                if any(word in token for word in fact_words):
                    token = token.translate(str.maketrans('','',string.punctuation))
                    f_out.write(token + '\n')

"""ChatBot"""

def create_response(sentence, noun, adjective):

  parent_directory = os.path.split(CWD)[0]
  dir_path = "content/files"
  filename = 'knowledge_base.txt'
  file_path = os.path.join(parent_directory, dir_path)
  file_path = os.path.join(file_path, filename)

  # knowledge base
  f = open(file_path, 'r', encoding='utf-8')
  text = f.read()
  text = text.lower()
  text = text.split('\n')

  # parts of speech tagger
  def POSfind(parsed): 
    pronoun = None
    noun = None
    adjective = None
    verb = None
    for sent in parsed.sentences:
        for w, p in sent.pos_tags:
          if p.startswith('VB'):
            verb = w
          elif p == 'JJ':
            adj = w
          elif p == 'NN':
            noun = w
          elif p == 'PRP' and w.lower() == 'you':
            pronoun = 'I'
          elif p == 'PRP' and w == 'I':
            pronoun = 'You'
    return pronoun, noun, adjective, verb

  # capitilize 'I'
  def capIs(sentence):
    icapped = []
    words = sentence.split(' ')
    for w in words:
        if w == 'i':
            w = 'I'
        if w == "i'm"
            w = "I'm"
        icapped.append(w)
    return ' '.join(icapped)
  
  # find match with noun and/or adjective
  for line in text:
      if noun is not None and adjective is not None:
          if noun in line and adjective in line:
              print(line)
              return line
      if noun is not None:
          if noun in line:
              return line
      if adjective is not None:
          if adjective in line:
              return line
  
  # tokenize and find match with any other word
  tokens = word_tokenize(sentence)
  stop_words = set(stopwords.words('english'))
  filtered_sentence = [w for w in tokens if not w in stop_words]
  for x in filtered_sentence:
      for line in text:
          if x in line:
              return line

# if greeting
GREETS = ("hello", "hi", "hey", "what's up", "whats up", "sup")
GREETRESP = ["yo", "whats poppin", "what up"]
def check_greeting(sentence):
    for word in sentence.words:
        if word.lower() in GREETS:
            return random.choice(GREETRESP)

# if exit
runst = True
EXITS = ["goodbye", "bye", "exit"]
EXITRESP = ["peace", "catch ya later", "bet see ya"]
def check_ending(sentence):
    global runst
    for word in sentence.words:
        if word.lower() in EXITS:
            runst = False
            return random.choice(EXITRESP)

# back up response reformulating user input
def construct_response(pronoun, noun, verb):
    resp = []
    if pronoun:
        resp.append(pronoun)
    if verb:
        verb_word = verb[0]
        if pronoun.lower() == 'you':
            resp.append("aren't really")
        else:
            resp.append(verb_word)
    if noun:
        pronoun = "an" if word[0] in 'aeiou' else "a"
        resp.append(pronoun + " " + noun)
    resp.append("smh")
    return ' '.join(resp)

REFERYOU = ["Kanye in my top 5.", "Donda album of the year no debate!"]
NULLRESP = [ "Kanye goated. Lets talk.", "Lemme tell ya somethin'!", "What do you know about Kanye?"]

def respond(userinp):
    icapped = capIs(userinp)
    parsed = TextBlob(icapped)
    pronoun, noun, adjective, verb = POSfind(parsed)

    # exit or greet
    resp = check_ending(parsed)
    if not resp:
        resp = check_greeting(parsed)
    
    # response from knowledge base
    if not resp:
        resp = create_response(userinp, noun, adjective)

    # last choice resps (random or reword)
    if not resp:
        if not pronoun:
            resp = random.choice(NULLRESP)
        elif pronoun == 'I' and not verb:
            resp = random.choice(REFERYOU)
        else:
            resp = construct_response(pronoun, noun, verb)  
    if not resp:
        resp = random.choice(NULLRESP)
    return resp

allusers = {}
ongouser = False 
def main():
    global runst
    global ongouser
    while runst:
        if not ongouser:
            user_name = input("> Aye im Kanye Bot. Whats your name?")
            allusers["user_name"] = user_name
            user_response = user_name
            ongouser = True
            intro_response = "> What up " +  allusers["user_name"] + ", whatcha need? "
            user_response = input(intro_response)
            response = respond(user_response)
            print(response)          
        else:
            user_response = input('> ')
            response = respond(user_response)
            print(response)          

if __name__ == "__main__":
    main()